#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import print_function
import os
os.environ['KERAS_BACKEND']='tensorflow'
from tensorflow.keras import backend as K
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import matplotlib
import pandas as pd
matplotlib.use('agg')
import matplotlib.pyplot as plt
import numpy as np
import csv
import h5py
import time
import shutil
from .Method import acc, SeqSelfAttention, FeedForward, LayerNormalization
from .Method import generate_arrays_from_file, picker
from .Method import DataGeneratorTest
np.warnings.filterwarnings('ignore')
import datetime
import ipdb
from tqdm import tqdm
from tensorflow.python.util import deprecation
deprecation._PRINT_DEPRECATION_WARNINGS = False


def tester(input_hdf5=None,
           input_csv=None,
           input_testset=None,
           input_model=None,
           output_name=None,              
           number_of_plots=100,
           estimate_uncertainty=False, 
           number_of_sampling=5,
           loss_weights=[0.05, 0.40, 0.55],
           loss_types=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],
           input_dimention=(20000, 3),
           batch_size=500,
           gpuid=None,
           gpu_limit=None):

    """
    
    Applies a trained model to a windowed waveform to perform both detection and picking at the same time.  


    Parameters
    ----------
    input_hdf5: str, default=None
        Path to an hdf5 file containing only one class of "data" with NumPy arrays containing 3 component waveforms each 1 min long.

    input_testset: npy, default=None
        Path to a NumPy file (automaticaly generated by the trainer) containing a list of trace names.        

    input_model: str, default=None
        Path to a trained model.
        
    output_name: str, default=None
        Output directory that will be generated. 

    number_of_plots: float, default=10
        The number of plots for detected events outputed for each station data.
        
    estimate_uncertainty: bool, default=False
        If True uncertainties in the output probabilities will be estimated.  
        
    number_of_sampling: int, default=5
        Number of sampling for the uncertainty estimation. 
               
    loss_weights: list, default=[0.03, 0.40, 0.58]
        Loss weights for detection, P picking, and S picking respectively.
             
    loss_types: list, default=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'] 
        Loss types for detection, P picking, and S picking respectively.
        
    input_dimention: tuple, default=(6000, 3)
        Loss types for detection, P picking, and S picking respectively.          

    batch_size: int, default=500 
        Batch size. This wont affect the speed much but can affect the performance. A value beteen 200 to 1000 is recommanded.

    gpuid: int, default=None
        Id of GPU used for the prediction. If using CPU set to None.
         
    gpu_limit: int, default=None
        Set the maximum percentage of memory usage for the GPU.
        
      
    Returns
    -------- 
    ./output_name/X_test_results.csv: A table containing all the detection, and picking results. Duplicated events are already removed.      
        
    ./output_name/X_report.txt: A summary of the parameters used for prediction and performance.
        
    ./output_name/figures: A folder containing plots detected events and picked arrival times. 
    
        
    """ 
              
         
    args = {
    "input_hdf5": input_hdf5,
    "input_csv": input_csv,
    "input_testset": input_testset,
    "input_model": input_model,
    "output_name": output_name,
    "number_of_plots": number_of_plots,
    "estimate_uncertainty": estimate_uncertainty,
    "number_of_sampling": number_of_sampling,
    "loss_weights": loss_weights,
    "loss_types": loss_types,
    "input_dimention": input_dimention,
    "batch_size": batch_size,
    "gpuid": gpuid,
    "gpu_limit": gpu_limit
    }  

    
    if args['gpuid']:           
        os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(args['gpuid'])
        tf.Session(config=tf.ConfigProto(log_device_placement=True))
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        config.gpu_options.per_process_gpu_memory_fraction = float(args['gpu_limit']) 
        K.tensorflow_backend.set_session(tf.Session(config=config))
    
    save_dir = os.path.join(os.getcwd(), str(args['output_name']))
    save_figs = os.path.join(save_dir, 'figures')
 
    if os.path.isdir(save_dir):
        shutil.rmtree(save_dir)  # 先递归删除原来的
    os.makedirs(save_figs) 
 
    test = np.load(args['input_testset'])    # 'test_trainer_outputs/test.npy'
    
    print('Loading the model ...', flush=True)        
    model = load_model(args['input_model'], custom_objects={'SeqSelfAttention': SeqSelfAttention, 
                                                         'FeedForward': FeedForward,
                                                         'LayerNormalization': LayerNormalization, 
                                                         'acc': acc                                                                            
                                                         })
               
    model.compile(loss = args['loss_types'],
                  loss_weights =  args['loss_weights'],           
                  optimizer = Adam(lr = 0.001),
                  metrics = ['acc'])

    
   
    print('Loading is complete!', flush=True)  
    print('Testing ...', flush=True)    
    print('Writting results into: " ' + str(args['output_name'])+' "', flush=True)
    
    start_training = time.time()          
    
    accTst = open(os.path.join(save_dir, 'acc.csv'), 'w')
    acc_writer = csv.writer(accTst, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    acc_writer.writerow(['P_acc', 'S_acc'])

    preTst = open(os.path.join(save_dir,'true&pred.csv'), 'w')   
    pred_writer = csv.writer(preTst, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    pred_writer.writerow(['P_true',
                            'P_pred',                              
                            'P_error',

                            'S_true',
                            'S_pred', 
                            'S_error',
                          ])
    
    csvTst = open(os.path.join(save_dir,'X_test_results.csv'), 'w')          
    test_writer = csv.writer(csvTst, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    test_writer.writerow([
                          'key', 
                          'number_of_detections',
                          'detection_probability',
                          'detection_uncertainty',
                          
                          'P_pick', 
                          'P_probability',
                          'P_uncertainty',
                          'P_error',
                          
                          'S_pick',
                          'S_probability',
                          'S_uncertainty', 
                          'S_error'
                          ])  
    csvTst.flush()        
        
    plt_n = 0
    list_generator = generate_arrays_from_file(test, args['batch_size'])   # list of keys 
    
    pbar_test = tqdm(total= int(np.ceil(len(test)/args['batch_size'])))   # 可视化进度条         
    for _ in range(int(np.ceil(len(test) / args['batch_size']))):
        pbar_test.update()
        new_list = next(list_generator)  # 遍历一个batch的keys

 
        params_test = {'file_name': str(args['input_hdf5']), 
                        'dim': args['input_dimention'][0],
                        'batch_size': len(new_list),
                        'n_channels': args['input_dimention'][-1]}     

        test_generator = DataGeneratorTest(new_list, **params_test)   # 一个batch
        # test_generator（i，20000，3）
        
        if args['estimate_uncertainty']:   # False
            pred_DD = []
            pred_PP = []
            pred_SS = []          
            for mc in range(args['number_of_sampling']):
                predD, predP, predS = model.predict_generator(generator=test_generator)
                pred_DD.append(predD)
                pred_PP.append(predP)               
                pred_SS.append(predS)
                    
            pred_DD = np.array(pred_DD).reshape(args['number_of_sampling'], len(new_list), params_test['dim'])
            pred_DD_mean = pred_DD.mean(axis=0)
            pred_DD_std = pred_DD.std(axis=0)  
            
            pred_PP = np.array(pred_PP).reshape(args['number_of_sampling'], len(new_list), params_test['dim'])
            pred_PP_mean = pred_PP.mean(axis=0)
            pred_PP_std = pred_PP.std(axis=0)      
            
            pred_SS = np.array(pred_SS).reshape(args['number_of_sampling'], len(new_list), params_test['dim'])
            pred_SS_mean = pred_SS.mean(axis=0)
            pred_SS_std = pred_SS.std(axis=0) 
                
        else:          
            pred_DD_mean, pred_PP_mean, pred_SS_mean = model.predict_generator(generator=test_generator)
              
            pred_DD_mean = pred_DD_mean.reshape(pred_DD_mean.shape[0], pred_DD_mean.shape[1])    # （200，20000）
            pred_PP_mean = pred_PP_mean.reshape(pred_PP_mean.shape[0], pred_PP_mean.shape[1]) 
            pred_SS_mean = pred_SS_mean.reshape(pred_SS_mean.shape[0], pred_SS_mean.shape[1]) 
            
            pred_DD_std = np.zeros((pred_DD_mean.shape))   # (200,20000)
            pred_PP_std = np.zeros((pred_PP_mean.shape))
            pred_SS_std = np.zeros((pred_SS_mean.shape))  

        test_set={}
        meta_test_set={}
        fl = h5py.File(args['input_hdf5'], 'r')
        csv_file = pd.read_csv(args['input_csv'], dtype={'key':str})
        
        for ID in new_list:   # list of keys for tests
            dataset = fl.get('earthquake/'+str(ID))    
            key = csv_file.key
            meta_dataset = csv_file.loc[key==ID]   
            # print(type(meta_dataset))                    #  <class 'pandas.core.frame.DataFrame'>
            test_set.update( {str(ID) : dataset}) 
            meta_test_set.update( {str(ID) : meta_dataset})

        
        for ts in range(pred_DD_mean.shape[0]):  # 遍历一个batch ts=0，。。。，199
            print('pred_DD_mean.shape:{}'.format(pred_DD_mean.shape))   # ：(200,20000)
            print('pred_DD_mean:{}'.format(pred_DD_mean))     # 非空
            print('pred_PP_mean:{}'.format(pred_PP_mean))   # 非空
            print('pred_SS_mean:{}'.format(pred_SS_mean))   # 非空
            print('ts in pred_DD_mean.shape[0]:{}'.format(ts))   #:200


            evi =  new_list[ts]    # 要预测的key，title
            dataset = test_set[evi] 
            meta_dataset =  meta_test_set[evi]
            
            try:
                spt = int((meta_dataset['p_pick']+30)*100); # 100HZ
                print('spt in test:{}'.format(spt))
            except Exception:     
                spt = None
                                
            try:
                sst = int((meta_dataset['s_pick']+30)*100);
                print('sst in test:{}'.format(sst))
            except Exception:     
                sst = None
            #ipdb.set_trace() 
            matches, pick_errors, yh3=picker(args, pred_DD_mean[ts], pred_PP_mean[ts], pred_SS_mean[ts],
                                                    pred_DD_std[ts], pred_PP_std[ts], pred_SS_std[ts], spt, sst) 
            print('matches:{}'.format(matches))   
            print('pick_errors:{}'.format(pick_errors))
            _output_writer_test(args, dataset, evi, test_writer, csvTst, matches, pick_errors)
            _output_writer_pred(args,evi,spt,sst, pred_writer, preTst, matches, pick_errors)   

            if plt_n < args['number_of_plots']:  
                print('pred_DD_mean[ts]:{}'.format(pred_DD_mean[ts]))                        
                _plotter(meta_dataset,
                            dataset,
                            evi,
                            args, 
                            save_figs, 
                            pred_DD_mean[ts], 
                            pred_PP_mean[ts],
                            pred_SS_mean[ts],
                            pred_DD_std[ts],
                            pred_PP_std[ts], 
                            pred_SS_std[ts],
                            matches)

            plt_n += 1
        filename = os.path.join(save_dir,'true&pred.csv')
        _output_writer_acc(args, matches, acc_writer, accTst,filename, pick_errors)

    
    accTst_total = open(os.path.join(save_dir, 'acc.csv'), 'a')
    acc_total_writer = csv.writer(accTst_total, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    acc_total_writer.writerow(['P_acc_total', 'S_acc_total'])
    filename = os.path.join(save_dir,'true&pred.csv')
    _output_writer_acc(args, matches, acc_total_writer, accTst_total, filename, pick_errors)


    end_training = time.time()  
    delta = end_training - start_training
    hour = int(delta / 3600)
    delta -= hour * 3600
    minute = int(delta / 60)
    delta -= minute * 60
    seconds = delta     



    with open(os.path.join(save_dir,'X_report.txt'), 'a') as the_file: 
        the_file.write('================== Overal Info =============================='+'\n')               
        the_file.write('date of report: '+str(datetime.datetime.now())+'\n')         
        the_file.write('input_hdf5: '+str(args['input_hdf5'])+'\n')            
        the_file.write('input_testset: '+str(args['input_testset'])+'\n')
        the_file.write('input_model: '+str(args['input_model'])+'\n')
        the_file.write('output_name: '+str(args['output_name'])+'\n')  
        the_file.write('================== Testing Parameters ======================='+'\n')  
        the_file.write('mode: '+str(args['mode'])+'\n')  
        the_file.write('finished the test in:  {} hours and {} minutes and {} seconds \n'.format(hour, minute, round(seconds, 2))) 
        the_file.write('loss_types: '+str(args['loss_types'])+'\n')
        the_file.write('loss_weights: '+str(args['loss_weights'])+'\n')
        the_file.write('batch_size: '+str(args['batch_size'])+'\n')
        the_file.write('total number of tests '+str(len(test))+'\n')
        the_file.write('gpuid: '+str(args['gpuid'])+'\n')
        the_file.write('gpu_limit: '+str(args['gpu_limit'])+'\n')             
        the_file.write('================== Other Parameters ========================='+'\n')            
        the_file.write('estimate uncertainty: '+str(args['estimate_uncertainty'])+'\n')
        the_file.write('number of Monte Carlo sampling: '+str(args['number_of_sampling'])+'\n')                      
        the_file.write('number_of_plots: '+str(args['number_of_plots'])+'\n')                        


def _output_writer_acc(args, matches, output_writer, csvfile, filename, pick_errors):
    """
    writes accuracy of testing results

    Parameters
    ----------
    args: dic
        A dictionary containing all of the input parameters.    
   
              
    output_writer: obj
        For writing out the detection/picking results in the CSV file.
        
    csvfile: obj
        For writing out the detection/picking results in the CSV file.  

    filename: obj
        For reading the filename

    """
    #ipdb.set_trace()
    with open(filename, 'r', newline='') as file :
        reader = csv.reader(file)
        next(reader) 
        total = 0
        for row in reader:
            if any(cell.strip() for cell in row):
                total += 1

    file = pd.read_csv(filename)
    P_error_data = file['P_error']
    S_error_data = file['S_error']
    P_error_arr = np.array(P_error_data)   # array of P_error 
    S_error_arr = np.array(S_error_data)
    P_error_arr = np.where(P_error_arr<=100, 1, 0)
    S_error_arr = np.where(S_error_arr<=100, 1, 0)
    P_count = sum(P_error_arr)
    S_count = sum(S_error_arr)

    P_acc = P_count/total
    S_acc = S_count/total

    output_writer.writerow( [
                            P_acc, 
                            S_acc,
                            
                            ]) 
    
    csvfile.flush() 


def _output_writer_pred(args,  
                        evi, 
                        spt,
                        sst,
                        output_writer, 
                        csvfile, 
                        matches, 
                        pick_errors,
                        ):
    
    """ 
    
    Writes the true & prediction indexes into a CSV file.

    Parameters
    ----------
    args: dic
        A dictionary containing all of the input parameters.    

    evi: str
        Trace name.    
           
    spt: int
        P arrival

    sst: int
        S arrival
        
    output_writer: obj
        For writing out the detection/picking results in the CSV file.
        
    csvfile: obj
        For writing out the detection/picking results in the CSV file.  

    matches: dic
        Contains the information for the detected and picked event.  
      
    pick_errors: dic
        Contains prediction errors for P and S picks.          
        
    Returns
    --------  
    true&pred.csv  
    
        
    """        
    
    
    numberOFdetections = len(matches)
    print('numberOFdetections:{}'.format(numberOFdetections))
    if numberOFdetections != 0: 
        
        P_pred = matches[list(matches)[0]][3]
        P_error = pick_errors[list(matches)[0]][0]
        
        S_pred = matches[list(matches)[0]][6] 
        S_error = pick_errors[list(matches)[0]][1]  
        
    else: 
       
        P_pred = None
        P_error = None
        
        S_pred = None
        S_error = None
    P_true = spt
    S_true = sst

    output_writer.writerow( [
                            P_true,
                            P_pred,                              
                            P_error,
                            S_true,
                            S_pred, 
                            S_error,
                            
                            ]) 
    
    csvfile.flush() 
    
    
def _output_writer_test(args, 
                        meta_dataset, 
                        evi, 
                        output_writer, 
                        csvfile, 
                        matches, 
                        pick_errors,
                        ):
    
    """ 
    
    Writes the detection & picking results into a CSV file.

    Parameters
    ----------
    args: dic
        A dictionary containing all of the input parameters.    
 
    meta_dataset: csvobj
        Dataset object of the trace.

    evi: str
        Trace name.    
              
    output_writer: obj
        For writing out the detection/picking results in the CSV file.
        
    csvfile: obj
        For writing out the detection/picking results in the CSV file.  

    matches: dic
        Contains the information for the detected and picked event.  
      
    pick_errors: dic
        Contains prediction errors for P and S picks.          
        
    Returns
    --------  
    X_test_results.csv  
    
        
    """        
    
    
    numberOFdetections = len(matches)
    print('numberOFdetections:{}'.format(numberOFdetections))
    if numberOFdetections != 0: 
        
        D_prob =  matches[list(matches)[0]][1]
        D_unc = matches[list(matches)[0]][2]

        P_arrival = matches[list(matches)[0]][3]
        P_prob = matches[list(matches)[0]][4] 
        P_unc = matches[list(matches)[0]][5] 
        P_error = pick_errors[list(matches)[0]][0]
        
        S_arrival = matches[list(matches)[0]][6] 
        S_prob = matches[list(matches)[0]][7] 
        S_unc = matches[list(matches)[0]][8]
        S_error = pick_errors[list(matches)[0]][1]  
        
        P_arrival = matches[list(matches)[0]][3]
        P_prob = matches[list(matches)[0]][4]
        S_arrival = matches[list(matches)[0]][6]
        S_prob = matches[list(matches)[0]][7]

    else: 
        D_prob = None
        D_unc = None 

        P_arrival = None
        P_prob = None
        P_unc = None
        P_error = None
        
        S_arrival = None
        S_prob = None 
        S_unc = None
        S_error = None
   
    if P_unc:
        P_unc = round(P_unc, 3)
    
    output_writer.writerow( [
                             
                            numberOFdetections,
                            D_prob,
                            D_unc,    
                            
                            P_arrival, 
                            P_prob,
                            P_unc,                             
                            P_error,
                            
                            S_arrival, 
                            S_prob,
                            S_unc,
                            S_error,
                            
                            ]) 
    
    csvfile.flush()   
    
    
    


def _plotter(meta_dataset, dataset, evi, args, save_figs, yh1, yh2, yh3, yh1_std, yh2_std, yh3_std, matches):
    

    """ 

    Parameters
    ----------
    meta_dataset: obj
        The csv obj containing a NumPy array of 3 component data and associated attributes.
    
    dataset: obj
        The hdf5 obj containing a NumPy array of 3 component data and associated attributes.

    evi: str
        Trace name.  

    args: dic
        A dictionary containing all of the input parameters. 

    save_figs: str
        Path to the folder for saving the plots. 

    yh1: 1D array
        Detection probabilities. 

    yh2: 1D array
        P arrival probabilities.   
      
    yh3: 1D array
        S arrival probabilities.  

    yh1_std: 1D array
        Detection standard deviations. 

    yh2_std: 1D array
        P arrival standard deviations.   
      
    yh3_std: 1D array
        S arrival standard deviations. 

    matches: dic
        {detection statr-time:[ detection end-time, detection probability, detectin uncertainty, P arrival, P probabiliy, P uncertainty, S arrival,  S probability, S uncertainty]}
          
        
    """ 
    try:
        spt = int((meta_dataset['p_pick']+30)*100); # 100HZ
        print('spt in potter:{}'.format(spt))
    except Exception:     
        spt = None
                        
    try:
        sst = int((meta_dataset['s_pick']+30)*100);
        print('sst in plotter:{}'.format(sst))
    except Exception:     
        sst = None
    
    predicted_P = []
    predicted_S = []
    if len(matches) >=1:

        for match, match_value in matches.items():
            print('match_value:{}'.format(match_value))
            if match_value[3]: 
                predicted_P.append(match_value[3])   # 
            else:
                predicted_P.append(None)
                
            if match_value[6]:
                predicted_S.append(match_value[6])
                
            else:
                predicted_S.append(None)   
    
    meta_data = np.array(meta_dataset)
    data = np.array(dataset)
    fig, axs = plt.subplots(4, 1, figsize=(7.5, 5), sharex=True)
    fig.suptitle(str(evi),y=0.92)
  
    axs[0].plot(data[:, 0], color='#1f77b4')
 
    pl = None
    sl = None       
    ppl = None
    ssl = None  
    
    pl = axs[0].axvline(int(spt),color='#9B59B6', alpha=0.9,linestyle='-', linewidth=2, label='Manual_P_Arrival')
    sl = axs[0].axvline(int(sst), color='#2ca02c', alpha=0.9, linestyle='-', linewidth=2, label='Manual_S_Arrival')
    
    if pl or sl:    
        axs[0].legend(loc = 'upper right', fontsize=9, borderaxespad=0.)     


    axs[1].plot(data[:, 1] , color='#1f77b4')  # 4行1列第2个图  
    pl = axs[1].axvline(int(spt),color='#9B59B6', alpha=0.9, linestyle='-', linewidth=2, label='Manual_P_Arrival')
    sl = axs[1].axvline(int(sst), color='#2ca02c', alpha=0.9, linestyle='-', linewidth=2, label='Manual_S_Arrival')
    
    if pl or sl:    
        axs[1].legend(loc = 'upper right', fontsize=9, borderaxespad=0.)    
    

    axs[2].plot(data[:, 2], color='#1f77b4')    # 第3个预测图
             
    if len(predicted_P) > 0: 
        for pt in predicted_P:
            if pt:
                ppl = axs[2].axvline(int(pt), color='#d62728', alpha=0.8, linestyle='-', linewidth=2, label='Predicted_P_Arrival')

    if len(predicted_S) > 0:  
        for st in predicted_S: 
            if st:
                ssl = axs[2].axvline(int(st), color='#ff7f0e', alpha=0.8, linestyle='-', linewidth=2, label='Predicted_S_Arrival')
   

    if ppl or ssl:    
        axs[2].legend(loc = 'upper right', fontsize=9, borderaxespad=0.) 

                
    x = np.linspace(0, data.shape[0], data.shape[0], endpoint=True)    # x等差数列，x
    print('x.shape:{}'.format(x.shape))
    
    if args['estimate_uncertainty']:                               
        axs[3].plot(x, yh1, color='#1f77b4', linestyle='--', linewidth=1.5, label='Detection')    # x(1,) yh1(20000,)
        lowerD = yh1-yh1_std
        upperD = yh1+yh1_std
        plt.fill_between(x, lowerD, upperD, alpha=0.2, color='#1f77b4')            

    else:
        axs[3].plot(x, yh1, color='#1f77b4', linestyle='--',  linewidth=1.5, label='Detection')                       
                          
    pred_DD_ind = K.argmax(yh1, axis=-1)# 概率最大的位置   [2961]   
    pred_PP_ind = K.argmax(yh2, axis=-1)   # P点概率最大的位置
    pred_SS_ind = K.argmax(yh3, axis=-1)

    yh2[pred_PP_ind] = 1
    yh2[0:pred_PP_ind-1] = 0
    yh2[pred_PP_ind+1:len(yh2)] = 0   # [0 0 ... 1 1 1 ... 0 0 0]

    yh3[pred_SS_ind] = 1
    yh3[0:pred_SS_ind-1] = 0
    yh3[pred_SS_ind+1:len(yh3)] = 0    # [0 0 ... 1 1 1 ... 0 0 0]

    axs[3].plot(x, yh2, color='#d62728', linestyle='-.', linewidth=1.5, label='P_probability')
    axs[3].plot(x, yh3, color='#ff7f0e', linestyle='dotted', linewidth=1.5, label='S_probability')

    axs[3].legend(loc = 'upper right', fontsize=9, borderaxespad=0.) 
    # 调整布局
    plt.tight_layout(rect=[0, 0, 1, 0.96])                     
    fig.savefig(os.path.join(save_figs, str(evi.split('/')[-1])+'.svg')) 
