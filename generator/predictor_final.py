#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""

@author: zhouyuxin
last update: 08/15/2024

"""

from __future__ import print_function
import os
os.environ['KERAS_BACKEND']='tensorflow'
from tensorflow.keras import backend as K
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import matplotlib
import pandas as pd
matplotlib.use('agg')
import matplotlib.pyplot as plt
import numpy as np
import csv
import h5py
import time
import platform
from os import listdir
import shutil
from .Method import acc, SeqSelfAttention, FeedForward, LayerNormalization
from .Method import generate_arrays_from_file, picker_prediction
from .Method import DataGeneratorTest
np.warnings.filterwarnings('ignore')
import datetime
import multiprocessing
import contextlib
import sys
from tqdm import tqdm
from tensorflow.python.util import deprecation
deprecation._PRINT_DEPRECATION_WARNINGS = False


def predictor(input_dir=None,
           input_hdf5=None,
           input_csv=None,
           input_testset=None,
           input_model=None,
           output_dir=None,
           detection_threshold=0.3,                
           P_threshold=0.1,
           S_threshold=0.1, 
           number_of_plots=100,
           estimate_uncertainty=False, 
           number_of_sampling=5,
           loss_weights=[0.05, 0.40, 0.55],
           loss_types=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],
           input_dimention=(20000, 3),
           normalization_mode='std',
           mode='generator',
           batch_size=500,
           gpuid=None,
           gpu_limit=None,
           number_of_cpus=5,
           use_multiprocessing=False,
              keepPS=True,
              allowonlyS=False,
              spLimit=200):

    """
    
    Applies a trained model to a windowed waveform to perform both detection and picking at the same time.  


    Parameters
    ----------
    input_hdf5: str, default=None
        Path to an hdf5 file containing only one class of "data" with NumPy arrays containing 3 component waveforms each 1 min long.

    input_testset: npy, default=None
        Path to a NumPy file (automaticaly generated by the trainer) containing a list of trace names.        

    input_model: str, default=None
        Path to a trained model.
        
    output_dir: str, default=None
        Output directory that will be generated. 
        
    detection_threshold : float, default=0.3
        A value in which the detection probabilities above it will be considered as an event.
          
    P_threshold: float, default=0.1
        A value which the P probabilities above it will be considered as P arrival.

    S_threshold: float, default=0.1
        A value which the S probabilities above it will be considered as S arrival.
               
    number_of_plots: float, default=10
        The number of plots for detected events outputed for each station data.
        
    estimate_uncertainty: bool, default=False
        If True uncertainties in the output probabilities will be estimated.  
        
    number_of_sampling: int, default=5
        Number of sampling for the uncertainty estimation. 
               
    loss_weights: list, default=[0.03, 0.40, 0.58]
        Loss weights for detection, P picking, and S picking respectively.
             
    loss_types: list, default=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'] 
        Loss types for detection, P picking, and S picking respectively.
        
    input_dimention: tuple, default=(6000, 3)
        Loss types for detection, P picking, and S picking respectively.          

    normalization_mode: str, default='std' 
        Mode of normalization for data preprocessing, 'max', maximum amplitude among three components, 'std', standard deviation.

    mode: str, default='generator'
        Mode of running. 'pre_load_generator' or 'generator'.
                      
    batch_size: int, default=500 
        Batch size. This wont affect the speed much but can affect the performance. A value beteen 200 to 1000 is recommanded.

    gpuid: int, default=None
        Id of GPU used for the prediction. If using CPU set to None.
         
    gpu_limit: int, default=None
        Set the maximum percentage of memory usage for the GPU.
        
      
    Returns
    -------- 
    ./output_dir/X_test_results.csv: A table containing all the detection, and picking results. Duplicated events are already removed.      
        
        
    ./output_dir/figures: A folder containing plots detected events and picked arrival times. 
    

    Notes
    --------
    Estimating the uncertainties requires multiple predictions and will increase the computational time. 
    
        
    """ 
              
         
    args = {
    "input_dir": input_dir,
    "input_hdf5": input_hdf5,
    "input_csv": input_csv,
    "input_testset": input_testset,
    "input_model": input_model,
    "output_dir": output_dir,
    "detection_threshold": detection_threshold,
    "P_threshold": P_threshold,
    "S_threshold": S_threshold,
    "number_of_plots": number_of_plots,
    "estimate_uncertainty": estimate_uncertainty,
    "number_of_sampling": number_of_sampling,
    "loss_weights": loss_weights,
    "loss_types": loss_types,
    "input_dimention": input_dimention,
    "normalization_mode": normalization_mode,
    "mode": mode,
    "batch_size": batch_size,
    "gpuid": gpuid,
    "gpu_limit": gpu_limit,
    "number_of_cpus": number_of_cpus,
    "use_multiprocessing": use_multiprocessing,
    "keepPS": keepPS,
    "allowonlyS": allowonlyS,
    "spLimit": spLimit   
    }  

    # 使用gpu
    availble_cpus = multiprocessing.cpu_count()
    if args['number_of_cpus'] > availble_cpus:
        args['number_of_cpus'] = availble_cpus
    
    if args['gpuid']:           
        os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(args['gpuid'])
        tf.Session(config=tf.ConfigProto(log_device_placement=True))
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        config.gpu_options.per_process_gpu_memory_fraction = float(args['gpu_limit']) 
        K.tensorflow_backend.set_session(tf.Session(config=config))
    
    # 避免不必要的print
    class DummyFile(object):
        file = None
        def __init__(self, file):
            self.file = file
    
        def write(self, x):
            # Avoid print() second call (useless \n)
            if len(x.rstrip()) > 0:
                tqdm.write(x, file=self.file)
    
    @contextlib.contextmanager
    def nostdout():
        save_stdout = sys.stdout
        sys.stdout = DummyFile(sys.stdout)
        yield
        sys.stdout = save_stdout
    
    print('============================================================================')
    print('Running TranSeis ')  

    
    print('Loading the model ...', flush=True)        
    model = load_model(args['input_model'], custom_objects={'SeqSelfAttention': SeqSelfAttention, 
                                                         'FeedForward': FeedForward,
                                                         'LayerNormalization': LayerNormalization, 
                                                         'acc': acc                                                                            
                                                         })
               
    model.compile(loss = args['loss_types'],
                  loss_weights =  args['loss_weights'],           
                  optimizer = Adam(lr = 0.001),
                  metrics = ['acc'])

    
   
    print('Loading is complete!', flush=True)  
    print('Predicting ...', flush=True)    

    
    out_dir = os.path.join(os.getcwd(), str(args['output_dir']))
    if os.path.isdir(out_dir):
        print('============================================================================')        
        print(f' *** {out_dir} already exists! Overwrite it!')
       
        shutil.rmtree(out_dir)  
    os.makedirs(out_dir) 
    start_training = time.time()          


    if args['input_dir']:
        # predcition fot QiaoJia
        if platform.system() == 'Windows': 
            station_list = [ev.split(".")[0] for ev in listdir(args["input_dir"]) ];  # input_dir = ../hdf5_merged     station_list=[QJ01,QJ02,...,QJ10]
        station_list = sorted(set(station_list))   # set去重，sort默认升序排列
        print(f"######### There are files for {len(station_list)} stations in {args['input_dir']} directory. #########", flush=True)
        # station_list = ['QJ01']
        
        for ct, st in enumerate(station_list):
            # # st = QJ01
            args["input_hdf5"] = args["input_dir"]+"/"+st+".hdf5"

            
            save_dir = os.path.join(out_dir, str(st)+'_outputs')
            save_figs = os.path.join(save_dir, 'figures') 
            if os.path.isdir(save_dir):
                shutil.rmtree(save_dir)  
            os.makedirs(save_dir) 
            if args['number_of_plots']:
                os.makedirs(save_figs) 
            csvPr_gen = open(os.path.join(save_dir,'X_prediction_results.csv'), 'w')          
            predict_writer = csv.writer(csvPr_gen, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
            predict_writer.writerow(['file_name', 
                                        'network',
                                        'station',
                                        'station_lat',
                                        'station_lon',
                                        'station_elv',
                                        'event_start_time',
                                        'event_end_time',
                                        'detection_probability',
                                        'detection_uncertainty', 
                                        'p_arrival_time',
                                        'p_probability',
                                        'p_uncertainty',
                                        's_arrival_time',
                                        's_probability',
                                        's_uncertainty',
                                            ]) 
            csvPr_gen.flush()
            print(f'========= Started working on {st}, {ct+1} out of {len(station_list)} ...', flush=True)
            
            start_Predicting = time.time()       
            plt_n = 0
            fl = h5py.File(args['input_hdf5'], 'r')   
            prediction_list = list_hdf5_groups(fl)  # ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']
            list_generator=generate_arrays_from_file(prediction_list, args['batch_size'])     
            total_iters = int(np.ceil(len(prediction_list)/args['batch_size']))
            pbar_test = tqdm(total= total_iters, ncols=100, file=sys.stdout)    
            for _ in range(total_iters):
                        
                pbar_test.update()
                new_list = next(list_generator)  # 遍历一个batch的keys

                

                params_prediction = {'file_name': str(args['input_hdf5']), 
                            'dim': args['input_dimention'][0],
                            'batch_size': len(new_list),
                            'n_channels': args['input_dimention'][-1]}     
        
                prediction_generator = DataGeneratorTest(new_list, **params_prediction)   # 一个batch
                # test_generator（i，20000，3）
                
                if args['estimate_uncertainty']:   # True
                    pred_DD = []
                    pred_PP = []
                    pred_SS = []          
                    for mc in range(args['number_of_sampling']):
                        predD, predP, predS = model.predict_generator(generator=prediction_generator)
                        pred_DD.append(predD)
                        pred_PP.append(predP)               
                        pred_SS.append(predS)
                            
                    pred_DD = np.array(pred_DD).reshape(args['number_of_sampling'], len(new_list), params_prediction['dim'])
                    pred_DD_mean = pred_DD.mean(axis=0)
                    pred_DD_std = pred_DD.std(axis=0)  
                    
                    pred_PP = np.array(pred_PP).reshape(args['number_of_sampling'], len(new_list), params_prediction['dim'])
                    pred_PP_mean = pred_PP.mean(axis=0)
                    pred_PP_std = pred_PP.std(axis=0)      
                    
                    pred_SS = np.array(pred_SS).reshape(args['number_of_sampling'], len(new_list), params_prediction['dim'])
                    pred_SS_mean = pred_SS.mean(axis=0)
                    pred_SS_std = pred_SS.std(axis=0) 
                        
                else:          
                    pred_DD_mean, pred_PP_mean, pred_SS_mean = model.predict_generator(generator=prediction_generator)
                    # pred_DD_mean:yh1 ; pred_PP_mean:yh2;  pred_SS_mean:yh3
                    # argmax
                    
                    
                    pred_DD_mean = pred_DD_mean.reshape(pred_DD_mean.shape[0], pred_DD_mean.shape[1])    # （200，20000）
                    pred_PP_mean = pred_PP_mean.reshape(pred_PP_mean.shape[0], pred_PP_mean.shape[1]) 
                    pred_SS_mean = pred_SS_mean.reshape(pred_SS_mean.shape[0], pred_SS_mean.shape[1]) 
                    
                    pred_DD_std = np.zeros((pred_DD_mean.shape))   # (200,20000)
                    pred_PP_std = np.zeros((pred_PP_mean.shape))
                    pred_SS_std = np.zeros((pred_SS_mean.shape))  
                

                



                pred_set={}

                
                for ID in new_list:   # list of keys for tests
                    dataset = fl.get('earthquake/'+str(ID))    
                    pred_set.update( {str(ID) : dataset}) 

                
                for ts in range(pred_DD_mean.shape[0]):  # 遍历一个batch ts=0，。。。，199


                    

                    evi =  new_list[ts]    # 要预测的key，title
                    dataset = pred_set[evi] 

                    matches=picker_prediction(args, pred_DD_mean[ts], pred_PP_mean[ts], pred_SS_mean[ts],
                                                        pred_DD_std[ts], pred_PP_std[ts], pred_SS_std[ts]) 
                    print('matches:{}'.format(matches))   


                    if plt_n < args['number_of_plots']:  
                        
                        _plotter( 
                                    dataset,
                                    evi,
                                    args, 
                                    save_figs, 
                                    pred_DD_mean[ts], 
                                    pred_PP_mean[ts],
                                    pred_SS_mean[ts],
                                    pred_DD_std[ts],
                                    pred_PP_std[ts], 
                                    pred_SS_std[ts],
                                    matches)
        
                    plt_n += 1


            end_training = time.time()  
            delta = end_training - start_training
            hour = int(delta / 3600)
            delta -= hour * 3600
            minute = int(delta / 60)
            delta -= minute * 60
            seconds = delta     

            dd = pd.read_csv(os.path.join(save_dir,'X_prediction_results.csv'))
            print(f'\n', flush=True)
            print(' *** Finished the prediction in: {} hours and {} minutes and {} seconds.'.format(hour, minute, round(seconds, 2)), flush=True)         
            print(' *** Detected: '+str(len(dd))+' events.', flush=True)
            print(' *** Wrote the results into --> " ' + str(save_dir)+' "', flush=True)

            with open(os.path.join(save_dir,'X_report.txt'), 'a') as the_file: 
                the_file.write('================== Overal Info =============================='+'\n')               
                the_file.write('date of report: '+str(datetime.datetime.now())+'\n')         
                the_file.write('input_hdf5: '+str(args['input_hdf5'])+'\n')            
                the_file.write('input_testset: '+str(args['input_testset'])+'\n')
                the_file.write('input_model: '+str(args['input_model'])+'\n')
                the_file.write('output_dir: '+str(args['output_dir'])+'\n')  
                the_file.write('================== Testing Parameters ======================='+'\n')  
                the_file.write('mode: '+str(args['mode'])+'\n')  
                the_file.write('finished the test in:  {} hours and {} minutes and {} seconds \n'.format(hour, minute, round(seconds, 2))) 
                the_file.write('loss_types: '+str(args['loss_types'])+'\n')
                the_file.write('loss_weights: '+str(args['loss_weights'])+'\n')
                the_file.write('batch_size: '+str(args['batch_size'])+'\n')
                the_file.write('total number of tests '+str(len(prediction_list))+'\n')
                the_file.write('gpuid: '+str(args['gpuid'])+'\n')
                the_file.write('gpu_limit: '+str(args['gpu_limit'])+'\n')             
                the_file.write('================== Other Parameters ========================='+'\n')            
                the_file.write('normalization_mode: '+str(args['normalization_mode'])+'\n')
                the_file.write('estimate uncertainty: '+str(args['estimate_uncertainty'])+'\n')
                the_file.write('number of Monte Carlo sampling: '+str(args['number_of_sampling'])+'\n')             
                the_file.write('detection_threshold: '+str(args['detection_threshold'])+'\n')            
                the_file.write('P_threshold: '+str(args['P_threshold'])+'\n')
                the_file.write('S_threshold: '+str(args['S_threshold'])+'\n')
                the_file.write('number_of_plots: '+str(args['number_of_plots'])+'\n')                        
    else:
        # 没有input_dir,直接读取input_hdf5.predict for DiTing2
        save_dir = os.path.join(out_dir, '_outputs')

        save_figs = os.path.join(save_dir, 'figures') 
        if os.path.isdir(save_dir):
            shutil.rmtree(save_dir)  
        os.makedirs(save_dir) 
        if args['number_of_plots']:
            os.makedirs(save_figs) 
        csvPr_gen = open(os.path.join(save_dir,'X_prediction_results.csv'), 'w')          
        predict_writer = csv.writer(csvPr_gen, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        predict_writer.writerow(['file_name', 
                                    'network',
                                    'station',
                                    'station_lat',
                                    'station_lon',
                                    'station_elv',
                                    'event_start_time',
                                    'event_end_time',
                                    'detection_probability',
                                    'detection_uncertainty', 
                                    'p_arrival_time',
                                    'p_probability',
                                    'p_uncertainty',
                                    's_arrival_time',
                                    's_probability',
                                    's_uncertainty',
                                        ])  
        csvPr_gen.flush()
        print('========= Started predicting on '+ str(args['input_hdf5']), flush=True)
        
        start_Predicting = time.time()       

        plt_n = 0

        fl = h5py.File(args['input_hdf5'], 'r')   
        prediction_list = list_hdf5_groups(fl)  # delete 'earthquake'
        list_generator=generate_arrays_from_file(prediction_list, args['batch_size'])   # a batch of names
        
        total_iters = int(np.ceil(len(prediction_list)/args['batch_size']))

        pbar_test = tqdm(total= total_iters, ncols=100, file=sys.stdout)  
        for bn in range(total_iters): 
            with nostdout():           
                pbar_test.update()
            
            new_list = next(list_generator)  
            params_prediction = {'file_name': str(args['input_hdf5']), 
                        'dim': args['input_dimention'][0],
                        'batch_size': len(new_list),
                        'n_channels': args['input_dimention'][-1]} 
            prediction_generator = DataGeneratorTest(new_list, **params_prediction)     # (20000,3)  {'input':(20000,3)}
            
            if args['estimate_uncertainty']:
                if not args['number_of_sampling'] or args['number_of_sampling'] <= 0:
                    print('please define the number of Monte Carlo sampling!')
                
                pred_DD = []
                pred_PP = []
                pred_SS = []          
                for mc in range(args['number_of_sampling']):
                    predD, predP, predS = model.predict_generator(generator = prediction_generator)
                    pred_DD.append(predD)
                    pred_PP.append(predP)               
                    pred_SS.append(predS)
                                    
                pred_DD = np.array(pred_DD).reshape(args['number_of_sampling'], len(new_list), params_prediction['dim'])
                pred_DD_mean = pred_DD.mean(axis=0)
                pred_DD_std = pred_DD.std(axis=0)  
                        
                pred_PP = np.array(pred_PP).reshape(args['number_of_sampling'], len(new_list), params_prediction['dim'])
                pred_PP_mean = pred_PP.mean(axis=0)
                pred_PP_std = pred_PP.std(axis=0)      
                            
                pred_SS = np.array(pred_SS).reshape(args['number_of_sampling'], len(new_list), params_prediction['dim'])
                pred_SS_mean = pred_SS.mean(axis=0)
                pred_SS_std = pred_SS.std(axis=0)                       
            else:          
                pred_DD_mean, pred_PP_mean, pred_SS_mean = model.predict_generator(generator = prediction_generator)

                pred_DD_mean = pred_DD_mean.reshape(pred_DD_mean.shape[0], pred_DD_mean.shape[1]) 
                pred_PP_mean = pred_PP_mean.reshape(pred_PP_mean.shape[0], pred_PP_mean.shape[1]) 
                pred_SS_mean = pred_SS_mean.reshape(pred_SS_mean.shape[0], pred_SS_mean.shape[1]) 
                            
                pred_DD_std = np.zeros((pred_DD_mean.shape))
                pred_PP_std = np.zeros((pred_PP_mean.shape))
                pred_SS_std = np.zeros((pred_SS_mean.shape))   

            
            pred_set={}
            for ID in new_list:
                dataset = fl.get(str(ID))   # (20000,)
                pred_set.update( {str(ID) : dataset})
            for ts in range(pred_DD_mean.shape[0]): 
                evi =  new_list[ts]    # 要预测的key，title
                dataset = pred_set[evi]
            
                matches=picker_prediction(args, pred_DD_mean[ts], pred_PP_mean[ts], pred_SS_mean[ts],
                                                    pred_DD_std[ts], pred_PP_std[ts], pred_SS_std[ts])    
   
                if plt_n < args['number_of_plots']:  
                     
                    _plotter(
                                dataset,
                                evi,
                                args, 
                                save_figs, 
                                pred_DD_mean[ts], 
                                pred_PP_mean[ts],
                                pred_SS_mean[ts],
                                pred_DD_std[ts],
                                pred_PP_std[ts], 
                                pred_SS_std[ts],
                                matches)
    
                plt_n += 1
                
        end_Predicting = time.time() 
        delta = (end_Predicting - start_Predicting) 
        hour = int(delta / 3600)
        delta -= hour * 3600
        minute = int(delta / 60)
        delta -= minute * 60
        seconds = delta   
            
        dd = pd.read_csv(os.path.join(save_dir,'X_prediction_results.csv'))
        print(f'\n', flush=True)
        print(' *** Finished the prediction in: {} hours and {} minutes and {} seconds.'.format(hour, minute, round(seconds, 2)), flush=True)         
        print(' *** Detected: '+str(len(dd))+' events.', flush=True)
        print(' *** Wrote the results into --> " ' + str(save_dir)+' "', flush=True)

        with open(os.path.join(save_dir,'X_report.txt'), 'a') as the_file:    
                the_file.write('================== Overal Info =============================='+'\n')               
                the_file.write('date of report: '+str(datetime.now())+'\n')         
                the_file.write('input_hdf5: '+str(args['input_hdf5'])+'\n')            
                the_file.write('input_model: '+str(args['input_model'])+'\n')
                the_file.write('output_dir: '+str(save_dir)+'\n')  
                the_file.write('================== Prediction Parameters ======================='+'\n')  
                the_file.write('finished the prediction in:  {} hours and {} minutes and {} seconds \n'.format(hour, minute, round(seconds, 2))) 
                the_file.write('detected: '+str(len(dd))+' events.'+'\n')                                       
                the_file.write('loss_types: '+str(args['loss_types'])+'\n')
                the_file.write('loss_weights: '+str(args['loss_weights'])+'\n')
                the_file.write('batch_size: '+str(args['batch_size'])+'\n')       
                the_file.write('================== Other Parameters ========================='+'\n')            
                the_file.write('normalization_mode: '+str(args['normalization_mode'])+'\n')
                the_file.write('estimate uncertainty: '+str(args['estimate_uncertainty'])+'\n')
                the_file.write('number of Monte Carlo sampling: '+str(args['number_of_sampling'])+'\n')             
                the_file.write('detection_threshold: '+str(args['detection_threshold'])+'\n')            
                the_file.write('P_threshold: '+str(args['P_threshold'])+'\n')
                the_file.write('S_threshold: '+str(args['S_threshold'])+'\n')
                the_file.write('number_of_plots: '+str(args['number_of_plots'])+'\n')                        
                the_file.write('use_multiprocessing: '+str(args['use_multiprocessing'])+'\n')            
                the_file.write('gpuid: '+str(args['gpuid'])+'\n')
                the_file.write('gpu_limit: '+str(args['gpu_limit'])+'\n')    
                the_file.write('keepPS: '+str(args['keepPS'])+'\n')
                the_file.write('allowonlyS: '+str(args['allowonlyS'])+'\n')  
                the_file.write('spLimit: '+str(args['spLimit'])+' seconds\n') 



def list_hdf5_groups(hdf5_file):
    """ list of group names of hdf5 file"""

    wave_ids = []

    group_list = hdf5_file['earthquake'].keys()

    for wave_id in group_list:
        wave_ids.append(wave_id)
    return wave_ids



    
    
    


def _plotter( dataset, evi, args, save_figs, yh1, yh2, yh3, yh1_std, yh2_std, yh3_std, matches):
    

    """ 
    
    Generates plots.

    Parameters
    ----------
    meta_dataset: obj
        The csv obj containing a NumPy array of 3 component data and associated attributes.
    
    dataset: obj
        The hdf5 obj containing a NumPy array of 3 component data and associated attributes.

    evi: str
        Trace name.  

    args: dic
        A dictionary containing all of the input parameters. 

    save_figs: str
        Path to the folder for saving the plots. 

    yh1: 1D array
        Detection probabilities. 

    yh2: 1D array
        P arrival probabilities.   
      
    yh3: 1D array
        S arrival probabilities.  

    yh1_std: 1D array
        Detection standard deviations. 

    yh2_std: 1D array
        P arrival standard deviations.   
      
    yh3_std: 1D array
        S arrival standard deviations. 

    matches: dic
        {detection statr-time:[ detection end-time, detection probability, detectin uncertainty, P arrival, P probabiliy, P uncertainty, S arrival,  S probability, S uncertainty]}
          
        
    """ 

    
    predicted_P = []
    predicted_S = []
    if len(matches) >=1:

        for match, match_value in matches.items():
            print('match_value:{}'.format(match_value))
            if match_value[3]: 
                predicted_P.append(match_value[3])   # 
            else:
                predicted_P.append(None)
                
            if match_value[6]:
                predicted_S.append(match_value[6])
                
            else:
                predicted_S.append(None)


    
    

    data = np.array(dataset)
    fig, axs = plt.subplots(4, 1, figsize=(7.5, 5), sharex=True)
    fig.suptitle(str(evi),y=0.92)
 
    # fig = plt.figure()
    # ax = fig.add_subplot(411)    
    
    # 第一个图     
    axs[0].plot(data[:, 0], color='#1f77b4')
    #plt.rcParams["figure.figsize"] = (8,5)
    #legend_properties = {'weight':'bold'}  
    # plt.title(str(evi))     # evi==key
    #plt.tight_layout()
    #ymin, ymax = axs[0].get_ylim() 
         
    ppl = None
    ssl = None  

    
    if len(predicted_P) > 0: 

        for pt in predicted_P:
            if pt:
                ppl = axs[0].axvline(int(pt), color='#d62728', linestyle='--', linewidth=1.5,  label='Predicted_P_Arrival')

    if len(predicted_S) > 0:  
        for st in predicted_S: 
            if st:
                ssl = axs[0].axvline(int(st), color='#ff7f0e', linestyle='--', linewidth=1.5,  label='Predicted_S_Arrival')
    if ppl or ssl:    
        axs[0].legend(loc='upper right', fontsize=9) 

    # 4行1列第2个图  
    axs[1].plot(data[:, 1] , color='#1f77b4')
    #plt.tight_layout()  # 调整间隔适应画布
                  
    
    if len(predicted_P) > 0: 

        for pt in predicted_P:
            if pt:
                ppl = axs[1].axvline(int(pt), color='#d62728', linestyle='--', linewidth=1.5,  label='Predicted_P_Arrival')


    if len(predicted_S) > 0:  
        for st in predicted_S: 
            if st:
                ssl = axs[1].axvline(int(st), color='#ff7f0e', linestyle='--', linewidth=1.5,  label='Predicted_S_Arrival')

    if ppl or ssl:    
        axs[1].legend(loc = 'upper right', fontsize=9,borderaxespad=0.)   
    

      # 第3个预测图
    axs[2].plot(data[:, 2], color='#1f77b4')   
    #plt.tight_layout() 
             
    if len(predicted_P) > 0: 

        for pt in predicted_P:
            if pt:
               ppl = axs[2].axvline(int(pt), color='#d62728', linestyle='--', linewidth=1.5,  label='Predicted_P_Arrival')

    if len(predicted_S) > 0:  
        for st in predicted_S: 
            if st:
                ssl = axs[2].axvline(int(st), color='#ff7f0e', linestyle='--', linewidth=1.5,  label='Predicted_S_Arrival')


    if ppl or ssl:    
        axs[2].legend(loc = 'upper right', fontsize=9,borderaxespad=0.) 

                
    # detection的图
    x = np.linspace(0, data.shape[0], data.shape[0], endpoint=True)    # x等差数列，x
    #print('x.shape:{}'.format(x.shape))
    
    if args['estimate_uncertainty']:                               
        axs[3].plot(x, yh1, color='#1f77b4', linestyle='--', linewidth=1.5, label='Detection')    # x(1,) yh1(20000,)
        lowerD = yh1-yh1_std
        upperD = yh1+yh1_std
        axs[3].fill_between(x, lowerD, upperD, color='#1f77b4', alpha=0.2)    
        
             
    else:
        axs[3].plot(x, yh1, color='#1f77b4', linestyle='--', alpha = 0.5, linewidth=1.5, label='Detection')                       
       
    

                          
    pred_DD_ind = K.argmax(yh1, axis=-1)# 概率最大的位置   [2961]   
    pred_PP_ind = K.argmax(yh2, axis=-1)   # P点概率最大的位置
    pred_SS_ind = K.argmax(yh3, axis=-1)

    yh2[pred_PP_ind] = 1
    yh2[0:pred_PP_ind-1] = 0
    yh2[pred_PP_ind+1:len(yh2)] = 0   # [0 0 ... 1 1 1 ... 0 0 0]

    yh3[pred_SS_ind] = 1
    yh3[0:pred_SS_ind-1] = 0
    yh3[pred_SS_ind+1:len(yh3)] = 0    # [0 0 ... 1 1 1 ... 0 0 0]
    
    axs[3].plot(x, yh2, color='#d62728', linestyle='-.', linewidth=1.5, label='P_probability')
    axs[3].plot(x, yh3, color='#ff7f0e', linestyle='dotted', linewidth=1.5, label='S_probability')
    #plt.tight_layout()       
    #axs[3].ylim((-0.1, 1.1))
    axs[3].legend(loc='upper right', fontsize=9) 
    # 调整布局
    plt.tight_layout(rect=[0, 0, 1, 0.96])                    
    fig.savefig(os.path.join(save_figs, str(evi.split('/')[-1])+'.svg')) 


    
    